{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0cfe07b014d4870367da8a0513530648d8087115be69e8698b1821f7bb106330b",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Binäre Klassifikation anhand eines Datensatzes zur Hautfarbenerkennung"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Thema\n",
    "In dieser Binären Klassifikation handelt es sich um die Vorhersage auf das Eintreffen einer Hautfarbe eines Menschen aus der Kombination der Primärfarben. Um das Herauszufinden wird ein Datensatz (der mehrere Farbkombination) enthält verwendet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Technische Vorbereitung\n",
    "Damit alle notwendigen Funktionen und Befehle zur Verfügung stehen, müssen zunächst einige Module mit ihren jeweiligen Klassen importiert werden. Dazu zählt pandas und scikit-learn. Alle Module werden für spätere Datenanalysemethoden genutzt.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 36,
   "outputs": []
  },
  {
   "source": [
    "## Analyse des Datensatzes\n",
    "Der Datensatz liegt hierbei in einer \".csv\" - Datei und beinhaltet 2500 Datensätze. Er ist in drei Spalten untergliedert. Die Spalte 0 entspricht der Farbe blau, Spalte 1 entspricht der Farbe grün, Spalte 2 entspricht der Farbe rot und die Spalte 3 beinhaltet die beiden Gruppen in Form von Ziffern. Dabei steht die 1 für Hautfarbe und die 2 definiert eine Farbkombination, die keine Hautfarbe ergibt.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     0    1    2  3\n0  176  174  126  2\n1  153  191  255  2\n2   57   59   23  2\n3  198  198  158  2\n4  178  175  131  2\n(2500, 4)\n\n                 0            1            2            3\ncount  2500.000000  2500.000000  2500.000000  2500.000000\nmean    124.656400   132.313600   125.663200     1.776000\nstd      61.985437    60.021465    71.977355     0.417005\nmin       0.000000     0.000000     0.000000     1.000000\n25%      70.000000    87.000000    82.000000     2.000000\n50%     138.000000   153.000000   129.000000     2.000000\n75%     175.000000   177.000000   168.000000     2.000000\nmax     255.000000   255.000000   255.000000     2.000000\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Skin_NonSkin.csv\", header=None)\n",
    "print(dataset.head())\n",
    "print(dataset.shape)\n",
    "print(\"\")\n",
    "print(dataset.describe())\n"
   ]
  },
  {
   "source": [
    "Sowohl die Farben- als auch die Gruppen-Spalte besitzen den Datentyp int64."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    int64\n1    int64\n2    int64\n3    int64\ndtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset.dtypes)\n"
   ]
  },
  {
   "source": [
    "Durch den folgenden Code wurde die Anzahl von Hautfarben bzw. keiner Hautfarbe in dem Datensatz gezählt. Dabei fällt auf, dass es sehr unausgeglichen ist und es sich bei den meisten Kombinationen um keine Hautfarbe handelt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n1     560\n2    1940\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.groupby(3).size())\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Trainings- und Testdaten\n",
    "Im nächsten Schritt werden die Daten in den Spalten nach Primärfarben und Hautfarbe gruppiert. Die Trainingsdaten werden für das Lernen der Muster und Zusammenhänge in den Daten verwendet. Der Algorithmus nutzt diese Daten, um daraus zu lernen. Bei den Testdaten handelt es sich um Daten mit der gleichen Wahrscheinlichkeitsverteilung. Diese Daten werden vom Algorithmus vorher nicht genutzt. Mit ihnen kann nachgeweisen werden, mit welcher Qualität der Algorithmus auf neue Daten reagieren wird. Die Verteilung splittet sich auf 80% Trainings- und 20% Testdaten."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2000, 3)\n(500, 3)\n"
     ]
    }
   ],
   "source": [
    "x = dataset.values[:,0:3].astype(float)\n",
    "y = dataset.values[:,3]\n",
    "\n",
    "x_train,y_train = x[:2000],y[:2000]\n",
    "x_test,y_test = x[2000:],y[2000:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "source": [
    "## Evaluierung der Algorithmen\n",
    "Im folgenden Code wird mit Hilfe der Genauigkeit der beste Algorithmus herausgefunden. Die Genauigkeit wird dabei anhand des Mittelwertes und der Standardabweichung gemessen. Es stellt sich heraus, dass der Gauß-Algorithmus und der Support-Vector-Machine-Klassifikator am besten geeignet sind."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LR: 0.915500 (0.018768)\nNB: 0.922000 (0.019261)\nSVM: 0.986500 (0.006344)\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "models.append((\"LR\", LogisticRegression()))\n",
    "models.append((\"NB\", GaussianNB()))\n",
    "models.append((\"SVM\", SVC()))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "source": [
    "## Evaluierung der Algorithmen mit Hilfe der Standardisierung\n",
    "In diesem Code werden alle Merkmale so transformiert, dass sie einen Mittelwert von 0 und eine Standardabweichung von 1 besitzen. Durch die Verwendung von pipelines wird Datenverlust vermieden. Dabei fällt auf, dass die Genauigkeit bei allen Algorithmen gestiegen ist. Der Gauß-Algorithmus und der Support-Vektor-Machine-Klassifikator haben weiterhin die größte Genauigkeit."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ScaledLR: 0.919250 (0.003902)\n",
      "ScaledNB: 0.924500 (0.005213)\n",
      "ScaledSVM: 0.994975 (0.001301)\n"
     ]
    }
   ],
   "source": [
    "pipelines = []\n",
    "pipelines.append((\"ScaledLR\", Pipeline([(\"Scaler\", StandardScaler()), (\"LR\", LogisticRegression())])))\n",
    "pipelines.append((\"ScaledNB\", Pipeline([(\"Scaler\", StandardScaler()), (\"NB\", GaussianNB())])))\n",
    "pipelines.append((\"ScaledSVM\", Pipeline([(\"Scaler\", StandardScaler()), (\"SVM\", SVC())])))\n",
    "\n",
    "results =[]\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n"
   ]
  },
  {
   "source": [
    "## Verbesserung der Algorithmen\n",
    "Durch das Verändern/Anpassen von Parametern für einen jeweiligen Algroithmus kann seine Genauigkeit erhöht werden.\n",
    "\n",
    "### Verbesserung des Support-Vector-Machine-Klassifikator\n",
    "Beim SVM lassen sich zwei Parameter anpassen. Zum einen der kernel und zum anderen der C-Wert. Der kernel versucht den Trainingsvektor in verschiedene höherdimensionale Räume zu überführen, damit die Vektoren sich leichter trennen lassen. Beim C-Wert wird dem Klassifikator mitgeteilt, wie sehr er eine Fehlklassifizierung vermeiden soll. Hierbei entsteht eine Genauigkeit von 99,75% mit dem C-Wert von 255 und dem Standard kernel rbf (Radial Basis Function)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best: 0.997500 using {'C': 255, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(x_train)\n",
    "standardX = scaler.transform(x_train)\n",
    "c_values = [0.1, 5, 25, 100, 255]\n",
    "kernel_values = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "model = SVC()\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=\"accuracy\", cv=kfold)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "source": [
    "## Random Forest\n",
    "Hierbei handelt es sich um einen Meta-Schätzer. Er passt mehrere Entscheidungsbaumklassifikatoren für verschiedene Teilstichproben des Datensatzes an. Dazu bildet er einen Mittelwert um die Genauigkeit zu verbessern. Des Weiteren gehört Random Forest zu den Bagging Mehtoden. Sie kombinieren verschiedene Vorhersagen anderer Algorithemen um eine genauere Vorhersage zu treffen. Am Ende ergibt sich eine Genauigkeit von 99,3%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RF: 0.993000 (0.005099)\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "msg = \"RF: %f (%f)\" % (cv_results.mean(), cv_results.std())\n",
    "print(msg)  "
   ]
  },
  {
   "source": [
    "## Vorhersage\n",
    "Die beste Genauigkeit erzielte der Support-Vektor-Machine-Klassifikator mit den Parametern: \n",
    "\n",
    "    C-Wert: 255\n",
    "    kernel: rbf\n",
    "Im folgenden Code wird der Klassifikator mit den Testdaten überprüft, um eine Genauigkeit der Vorhersage neuer Werte zu treffen. Die Werte ähneln der vordefinierten Genauigkeit des Support-Vektor-Machine-Klassifikator, sie beträgt 100%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n[[128   0]\n [  0 372]]\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00       128\n           2       1.00      1.00      1.00       372\n\n    accuracy                           1.00       500\n   macro avg       1.00      1.00      1.00       500\nweighted avg       1.00      1.00      1.00       500\n\n"
     ]
    }
   ],
   "source": [
    "model = SVC(C=255, kernel=\"rbf\")\n",
    "model.fit(standardX, y_train)\n",
    "validateX = scaler.transform(x_test)\n",
    "predictions = model.predict(validateX)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Quellen\n",
    "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/\n",
    "\n",
    "Brownlee, Jason (2018): Machine Learning Mastery with Python, S. 144 - 161 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}